import os
import sys
import csv
import re
from flask import Flask, request, jsonify
from flask_cors import CORS

# ç¡®ä¿èƒ½å¯¼å…¥é¡¹ç›®æ ¹ç›®å½•çš„ util
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from util.handle_csv import save_to_csv, fieldnames, generate_job_id

app = Flask(__name__)
CORS(app)  # å…è®¸æµè§ˆå™¨æ’ä»¶è·¨åŸŸè°ƒç”¨

JOBS_META_FILE = os.path.join(os.path.dirname(__file__), "csv_file", "jobs_meta.csv")
JOBS_UPDATED_FILE = os.path.join(os.path.dirname(__file__), "csv_file", "jobs_meta_updated.csv")

@app.route('/upload_list', methods=['POST'])
def upload_list():
    data = request.json
    jobs = data.get('jobs', [])
    if jobs:
        save_to_csv(JOBS_META_FILE, jobs)
        return jsonify({"success": True, "count": len(jobs)})
    return jsonify({"success": False, "message": "No jobs provided"})

@app.route('/upload_detail', methods=['POST'])
def upload_detail():
    item = request.json
    if not item or 'source_url' not in item:
        return jsonify({"success": False, "message": "Invalid detail data"})
    
    # 1. æå…¶ä¸¥æ ¼åœ°æ¸…æ´—å½“å‰ URLï¼Œç”¨äºåŒ¹é…
    def clean_url(u):
        return u.split('?')[0].split('#')[0].strip().rstrip('/')

    current_url = clean_url(item.get('source_url', ''))
    job_id = generate_job_id(current_url)
    
    # 2. ä» jobs_meta.csv ä¸­å¯»æ‰¾åŸå§‹ä¿¡æ¯
    meta_info = {}
    if os.path.exists(JOBS_META_FILE):
        try:
            with open(JOBS_META_FILE, "r", encoding="utf-8-sig") as f:
                reader = csv.DictReader(f)
                for row in reader:
                    # åŒæ—¶å¯¹æ¯” ID å’Œæ¸…æ´—åçš„ URL
                    row_url = clean_url(row.get("source_url", ""))
                    if row.get("_id") == job_id or row_url == current_url:
                        meta_info = row
                        break
        except Exception as e:
            print(f"è¯»å– meta æ–‡ä»¶å‡ºé”™: {e}")

    # 3. æ„é€ å®Œæ•´å­—æ®µï¼Œæ˜ç¡®ä¼˜å…ˆçº§
    full_item = {field: "" for field in fieldnames}
    
    # é»˜è®¤å€¼
    full_item['_id'] = job_id
    full_item['source_url'] = current_url
    # --- æ ¸å¿ƒä¿®å¤ï¼šsource_name ç»Ÿä¸€å¤„ç† ---
    full_item['source_name'] = meta_info.get('source_name') or item.get('source_name') or "BOSSç›´è˜"
    if full_item['source_name'] == "BOSSç›´è˜":
        full_item['source_name_english'] = "BOSS Zhipin"
    elif full_item['source_name'] == "æ™ºè”æ‹›è˜":
        full_item['source_name_english'] = "Zhaopin"
    full_item['is_remote'] = "1"

    # --- æ ¸å¿ƒä¿®å¤ï¼šæ ‡é¢˜å’ŒåŸºç¡€å­—æ®µä¼˜å…ˆä» meta æ‹¿ ---
    raw_title = meta_info.get('title') or item.get('title') or "æœªçŸ¥èŒä½"
    
    # æ¸…ç†æ ‡é¢˜ä¸­çš„è–ªèµ„æ®‹ç•™ï¼ˆä¾‹å¦‚ "èŒä½åç§° 15-25K"ï¼‰
    # åŒ¹é…ç±»ä¼¼ 10-20K, 15è–ª, 500-800å…ƒ/å¤© ç­‰æ¨¡å¼
    salary_patterns = [
        r'\s*[\d\.\-kK]+[kK]', # 15K, 15-25k
        r'\s*[\d\-]+è–ª',       # 13è–ª
        r'\s*[\d\.\-]+å…ƒ/.*',  # 500å…ƒ/å¤©
    ]
    clean_title = raw_title
    for pattern in salary_patterns:
        clean_title = re.sub(pattern, '', clean_title)
    
    full_item['title'] = clean_title.strip()
    
    for field in ['team', 'salary', 'city', 'experience']:
        full_item[field] = meta_info.get(field) or item.get(field) or ""

    # è¯¦æƒ…ä¿¡æ¯åˆ™å¿…é¡»ä½¿ç”¨æ’ä»¶æ–°æŠ“å–çš„
    full_item['description'] = item.get('description') or ""
    if item.get('keywords'):
        full_item['summary'] = ",".join(item['keywords'])
    elif meta_info.get('summary'):
        full_item['summary'] = meta_info['summary']

    # å‘å¸ƒæ—¶é—´
    full_item['createdAt'] = item.get('createdAt') or meta_info.get('createdAt') or ""

    # å»æ‰æ‰€æœ‰å€¼çš„ä¸¤ç«¯ç©ºæ ¼
    for k in full_item:
        if isinstance(full_item[k], str):
            full_item[k] = full_item[k].strip()
        
    save_to_csv(JOBS_UPDATED_FILE, [full_item])
    print(f"âœ… å·²åŒæ­¥è¯¦æƒ…: {full_item['title']} (ID: {job_id[:8]})")
    return jsonify({"success": True})

@app.route('/get_next_url', methods=['POST'])
def get_next_url():
    """å‘Šè¯‰æ’ä»¶ä¸‹ä¸€ä¸ªè¦æŠ“å–çš„è¯¦æƒ…é¡µ URL"""
    processed_ids = set()
    if os.path.exists(JOBS_UPDATED_FILE):
        try:
            with open(JOBS_UPDATED_FILE, "r", encoding="utf-8-sig") as f:
                for row in csv.DictReader(f):
                    if row.get("_id"):
                        processed_ids.add(row["_id"])
        except: pass
    
    if os.path.exists(JOBS_META_FILE):
        try:
            with open(JOBS_META_FILE, "r", encoding="utf-8-sig") as f:
                for row in csv.DictReader(f):
                    job_id = row.get("_id")
                    if job_id and job_id not in processed_ids:
                        return jsonify({"url": row.get("source_url")})
        except: pass
    
    return jsonify({"url": None})

if __name__ == '__main__':
    print("\n" + "="*60)
    print("ğŸš€ BOSS Bridge Server å·²å¯åŠ¨")
    print("åœ°å€: http://127.0.0.1:5000 (æˆ– http://localhost:5000)")
    print("è¯´æ˜: å¦‚æœæ’ä»¶è¿ä¸ä¸Šï¼Œè¯·å°è¯•åœ¨æ¢¯å­è®¾ç½®ä¸­æ’é™¤ 127.0.0.1")
    print("="*60 + "\n")
    # ç›‘å¬ 0.0.0.0 ä»¥ç¡®ä¿æ— è®ºæ’ä»¶ç”¨ä»€ä¹ˆåœ°å€éƒ½èƒ½è¿ä¸Š
    app.run(host='0.0.0.0', port=5000, debug=False)
