import os
import sys
import csv
import re
from flask import Flask, request, jsonify
from flask_cors import CORS

# ç¡®ä¿èƒ½å¯¼å…¥é¡¹ç›®æ ¹ç›®å½•çš„ util
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from util.handle_csv import save_to_csv, fieldnames, generate_job_id

app = Flask(__name__)
CORS(app)  # å…è®¸æµè§ˆå™¨æ’ä»¶è·¨åŸŸè°ƒç”¨

ZHILIAN_DIR = os.path.dirname(os.path.abspath(__file__))
JOBS_META_FILE = os.path.join(ZHILIAN_DIR, "csv_file", "jobs_meta.csv")
JOBS_UPDATED_FILE = os.path.join(ZHILIAN_DIR, "csv_file", "jobs_meta_updated.csv")

@app.route('/upload_list', methods=['POST'])
def upload_list():
    data = request.json
    jobs = data.get('jobs', [])
    if jobs:
        save_to_csv(JOBS_META_FILE, jobs)
        return jsonify({"success": True, "count": len(jobs)})
    return jsonify({"success": False, "message": "No jobs provided"})

@app.route('/upload_detail', methods=['POST'])
def upload_detail():
    item = request.json
    if not item or 'source_url' not in item:
        return jsonify({"success": False, "message": "Invalid detail data"})
    
    # 1. æå…¶ä¸¥æ ¼åœ°æ¸…æ´—å½“å‰ URLï¼Œç”¨äºåŒ¹é…
    def clean_url(u):
        if not u: return ""
        # å»æ‰åè®®å¤´ (http/https)ã€æŸ¥è¯¢å‚æ•°ã€é”šç‚¹ã€æœ«å°¾æ–œæ ï¼Œå¹¶è½¬å°å†™
        u = u.replace('https://', '').replace('http://', '')
        return u.split('?')[0].split('#')[0].strip().rstrip('/').lower()

    current_url_raw = item.get('source_url', '')
    current_url_cleaned = clean_url(current_url_raw)
    
    # 2. ä» jobs_meta.csv ä¸­å¯»æ‰¾åŸå§‹ä¿¡æ¯ï¼Œä¼˜å…ˆä½¿ç”¨ meta é‡Œçš„ ID
    meta_info = {}
    matched_job_id = None
    if os.path.exists(JOBS_META_FILE):
        try:
            with open(JOBS_META_FILE, "r", encoding="utf-8-sig") as f:
                reader = csv.DictReader(f)
                for row in reader:
                    row_url_cleaned = clean_url(row.get("source_url", ""))
                    if row_url_cleaned == current_url_cleaned:
                        meta_info = row
                        matched_job_id = row.get("_id")
                        break
        except Exception as e:
            print(f"è¯»å– meta æ–‡ä»¶å‡ºé”™: {e}")

    # å¦‚æœ meta é‡Œæ²¡æ‰¾åˆ°ï¼Œå†æ ¹æ®å½“å‰ URL ç”Ÿæˆä¸€ä¸ª ID
    job_id = matched_job_id or generate_job_id(current_url_raw)
    
    full_item = {field: "" for field in fieldnames}
    
    full_item['_id'] = job_id
    full_item['source_url'] = current_url_raw # ä¿æŒåŸå§‹ URL
    full_item['source_name'] = "æ™ºè”æ‹›è˜"
    full_item['source_name_english'] = "Zhilian Zhaopin"
    full_item['type'] = "å›½å†…"
    full_item['is_remote'] = "1"

    raw_title = meta_info.get('title') or item.get('title') or "æœªçŸ¥èŒä½"
    
    salary_patterns = [
        r'\s*[\d\.\-kK]+[kK]',
        r'\s*[\d\-]+è–ª',
        r'\s*[\d\.\-]+å…ƒ/.*',
    ]
    clean_title = raw_title
    for pattern in salary_patterns:
        clean_title = re.sub(pattern, '', clean_title)
    
    full_item['title'] = clean_title.strip()
    
    from utils import is_valid_experience, convert_salary_to_english
    for field in ['team', 'salary', 'city']:
        full_item[field] = meta_info.get(field) or item.get(field) or ""
    
    # å¯¹è–ªèµ„è¿›è¡Œå½’ä¸€åŒ–å¤„ç†
    full_item['salary'] = convert_salary_to_english(full_item['salary'], to_english=False)
    
    # å¯¹ç»éªŒå­—æ®µè¿›è¡Œå½’ä¸€åŒ–å¤„ç†
    raw_exp = meta_info.get('experience') or item.get('experience') or ""
    full_item['experience'] = is_valid_experience(raw_exp)

    full_item['description'] = item.get('description') or ""
    if item.get('keywords'):
        full_item['summary'] = ",".join(item['keywords'])
    elif meta_info.get('summary'):
        full_item['summary'] = meta_info['summary']

    full_item['createdAt'] = item.get('createdAt') or meta_info.get('createdAt') or ""

    for k in full_item:
        if isinstance(full_item[k], str):
            full_item[k] = full_item[k].strip()
        
    save_to_csv(JOBS_UPDATED_FILE, [full_item])
    print(f"âœ… å·²åŒæ­¥è¯¦æƒ…: {full_item['title']} (ID: {job_id[:8]})")
    return jsonify({"success": True})

@app.route('/get_next_url', methods=['POST'])
def get_next_url():
    """æ”¹è¿›çš„ä»»åŠ¡è·å–é€»è¾‘ï¼šæ— è§†åè®®å¤´ã€æ”¯æŒåŒé‡æ ¡éªŒ"""
    def clean_url(u):
        if not u: return ""
        u = u.replace('https://', '').replace('http://', '')
        return u.split('?')[0].split('#')[0].strip().rstrip('/').lower()

    processed_keys = set()
    if os.path.exists(JOBS_UPDATED_FILE):
        try:
            with open(JOBS_UPDATED_FILE, "r", encoding="utf-8-sig") as f:
                for row in csv.DictReader(f):
                    jid = row.get("_id")
                    url = clean_url(row.get("source_url"))
                    if jid: processed_keys.add(jid)
                    if url: processed_keys.add(url)
        except Exception as e:
            print(f"è¯»å–å·²å¤„ç†æ–‡ä»¶å‡ºé”™: {e}")
    
    if os.path.exists(JOBS_META_FILE):
        try:
            with open(JOBS_META_FILE, "r", encoding="utf-8-sig") as f:
                for row in csv.DictReader(f):
                    job_id = row.get("_id")
                    raw_url = row.get("source_url")
                    url = clean_url(raw_url)
                    
                    if job_id in processed_keys or url in processed_keys:
                        continue
                    
                    print(f"ğŸ¯ æ™ºè”ï¼šæ´¾å‘ä¸‹ä¸€ä¸ªä»»åŠ¡ -> {raw_url}")
                    return jsonify({"success": True, "url": raw_url})
        except Exception as e:
            print(f"è¯»å– meta æ–‡ä»¶å‡ºé”™: {e}")
    
    print("ğŸ æ™ºè”æ‹›è˜ï¼šæ‰€æœ‰è¯¦æƒ…é¡µå·²åŒæ­¥å®Œæˆ")
    return jsonify({"success": True, "url": None})

if __name__ == '__main__':
    print("\n" + "="*60)
    print("ğŸš€ æ™ºè”æ‹›è˜ Bridge Server å·²å¯åŠ¨")
    print("åœ°å€: http://127.0.0.1:5001")
    print("="*60 + "\n")
    app.run(host='0.0.0.0', port=5001, debug=False)
